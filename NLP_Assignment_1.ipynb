{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5xbWjvbnzEs1wWgKOu0vh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish000777/LP-6/blob/main/NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkolgSLRL6IW",
        "outputId": "95bac1d7-df30-4a07-cab7-a072d8432a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download necessary resources\n",
        "nltk.download('averaged_perceptron_tagger')  # For MWE tokenization\n",
        "nltk.download('wordnet')  # For lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This is a sample sentence, demonstrating various tokenization techniques!\""
      ],
      "metadata": {
        "id": "rzfhy5DOMyCc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whitespace_tokens = nltk.word_tokenize(sentence)\n",
        "print(\"Whitespace tokens:\", whitespace_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD_UrtDzM3Ht",
        "outputId": "dbe91453-fef7-4d4d-f546-282aa49fdfe4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace tokens: ['This', 'is', 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'techniques', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "punctuation_tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
        "punctuation_tokens = punctuation_tokenizer.tokenize(sentence)\n",
        "print(\"Punctuation-based tokens:\", punctuation_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59XHXY1CNhW7",
        "outputId": "fe450469-ffd5-4a51-9d40-d59903ce82c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation-based tokens: ['This', 'is', 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'techniques', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(sentence)\n",
        "print(\"Treebank tokens:\", treebank_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7Dt3gElNpoW",
        "outputId": "c7cf9cd2-6ea2-44b3-b121-521e71a46b5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank tokens: ['This', 'is', 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'techniques', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(sentence)\n",
        "print(\"Tweet tokens:\", tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA13scnLNt3I",
        "outputId": "9597a8ec-90bf-40ba-8862-b4c39327b4a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet tokens: ['This', 'is', 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'techniques', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe_tokenizer = MWETokenizer([('this is', 'this_is'), ('sample sentence', 'sample_sentence')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(sentence.split())\n",
        "print(\"MWE tokens:\", mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8OdvvmoNzGn",
        "outputId": "bb24c9bf-a8bb-4d36-a074-67a70361fd51"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MWE tokens: ['This', 'is', 'a', 'sample', 'sentence,', 'demonstrating', 'various', 'tokenization', 'techniques!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "porter_stemmed_tokens = [porter_stemmer.stem(token) for token in whitespace_tokens]\n",
        "print(\"Porter stemmed tokens:\", porter_stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytx6g6TeN6sj",
        "outputId": "36802755-090b-4c1c-b131-8cfb32db6d51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stemmed tokens: ['thi', 'is', 'a', 'sampl', 'sentenc', ',', 'demonstr', 'variou', 'token', 'techniqu', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "snowball_stemmed_tokens = [snowball_stemmer.stem(token) for token in whitespace_tokens]\n",
        "print(\"Snowball stemmed tokens:\", snowball_stemmed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXAoU0PFOKw1",
        "outputId": "4d545140-9ffb-4f53-a0f8-27c2ac6d1c89"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball stemmed tokens: ['this', 'is', 'a', 'sampl', 'sentenc', ',', 'demonstr', 'various', 'token', 'techniqu', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in whitespace_tokens]\n",
        "print(\"Lemmatized tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lyzo4mJObr4",
        "outputId": "51f5fb38-2692-4fc9-9529-f01df279aab4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens: ['This', 'is', 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'technique', '!']\n"
          ]
        }
      ]
    }
  ]
}