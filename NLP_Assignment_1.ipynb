{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6/FtRpPb3B7lU1jo2WZoY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashish000777/LP-6/blob/main/NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkolgSLRL6IW",
        "outputId": "95bac1d7-df30-4a07-cab7-a072d8432a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download necessary resources\n",
        "nltk.download('averaged_perceptron_tagger')  # For MWE tokenization\n",
        "nltk.download('wordnet')  # For lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"@It's a sample sentence, demonstrating various tokenization techniques!\""
      ],
      "metadata": {
        "id": "rzfhy5DOMyCc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whitespace_tokens = nltk.word_tokenize(sentence)\n",
        "print(\"Whitespace tokens:\", whitespace_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD_UrtDzM3Ht",
        "outputId": "3723b6fe-c5bd-436c-814f-09723574fa86"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace tokens: ['@', '#', 'It', \"'s\", 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'techniques', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "punctuation_tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
        "punctuation_tokens = punctuation_tokenizer.tokenize(sentence)\n",
        "print(\"Punctuation-based tokens:\", punctuation_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59XHXY1CNhW7",
        "outputId": "28643d66-3ec2-44ca-ce95-a8040864103c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation-based tokens: ['@', '#', 'It', \"'\", 's', 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'techniques', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(sentence)\n",
        "print(\"Treebank tokens:\", treebank_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7Dt3gElNpoW",
        "outputId": "ae5033e2-6950-4e53-d72d-a076a3924679"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank tokens: ['@', 'It', \"'s\", 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'techniques', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(sentence)\n",
        "print(\"Tweet tokens:\", tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA13scnLNt3I",
        "outputId": "761c56fc-ae12-49fa-8a97-ad5419df4c3e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet tokens: ['@It', \"'\", 's', 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'techniques', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe_tokenizer = MWETokenizer([('this is', 'this_is'), ('sample sentence', 'sample_sentence')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(sentence.split())\n",
        "print(\"MWE tokens:\", mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8OdvvmoNzGn",
        "outputId": "77a1744c-f7c3-4e55-ef32-f69aca99b803"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MWE tokens: ['@', '#', \"It's\", 'a', 'sample', 'sentence,', 'demonstrating', 'various', 'tokenization', 'techniques!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "porter_stemmed_tokens = [porter_stemmer.stem(token) for token in whitespace_tokens]\n",
        "print(\"Porter stemmed tokens:\", porter_stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytx6g6TeN6sj",
        "outputId": "2fb6340a-11bf-4655-8952-05e58180f813"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stemmed tokens: ['@', '#', 'it', \"'s\", 'a', 'sampl', 'sentenc', ',', 'demonstr', 'variou', 'token', 'techniqu', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "snowball_stemmed_tokens = [snowball_stemmer.stem(token) for token in whitespace_tokens]\n",
        "print(\"Snowball stemmed tokens:\", snowball_stemmed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXAoU0PFOKw1",
        "outputId": "089c94e0-f1fd-496f-e30d-2bfd8695b434"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball stemmed tokens: ['@', '#', 'it', \"'s\", 'a', 'sampl', 'sentenc', ',', 'demonstr', 'various', 'token', 'techniqu', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in whitespace_tokens]\n",
        "print(\"Lemmatized tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lyzo4mJObr4",
        "outputId": "7e759fe6-0a01-4372-d536-d366040c84c9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens: ['@', '#', 'It', \"'s\", 'a', 'sample', 'sentence', ',', 'demonstrating', 'various', 'tokenization', 'technique', '!']\n"
          ]
        }
      ]
    }
  ]
}